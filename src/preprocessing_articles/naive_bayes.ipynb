{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GetOldTweets3 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (0.0.11)\n",
      "Requirement already satisfied: pyquery>=1.2.10 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from GetOldTweets3) (1.4.1)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from GetOldTweets3) (4.5.0)\n",
      "Requirement already satisfied: cssselect>0.7.9 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from pyquery>=1.2.10->GetOldTweets3) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install GetOldTweets3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>twitter</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sherrod Brown</td>\n",
       "      <td>SenSherrodBrown</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maria Cantwell</td>\n",
       "      <td>SenatorCantwell</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Benjamin L. Cardin</td>\n",
       "      <td>SenatorCardin</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thomas R. Carper</td>\n",
       "      <td>SenatorCarper</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robert P. Casey, Jr.</td>\n",
       "      <td>SenBobCasey</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name          twitter     party\n",
       "0         Sherrod Brown  SenSherrodBrown  Democrat\n",
       "1        Maria Cantwell  SenatorCantwell  Democrat\n",
       "2    Benjamin L. Cardin    SenatorCardin  Democrat\n",
       "3      Thomas R. Carper    SenatorCarper  Democrat\n",
       "4  Robert P. Casey, Jr.      SenBobCasey  Democrat"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame, read_csv\n",
    "import pandas as pd \n",
    "twitter_url_reps = r'reps.csv'\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "acc_df = pd.read_csv(twitter_url_reps)\n",
    "df2 = acc_df[['name', 'twitter', 'party']]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./tweets/tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>twitter</th>\n",
       "      <th>party</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sherrod Brown</td>\n",
       "      <td>SenSherrodBrown</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>This year, #WorkersMemorialDay is particularly...</td>\n",
       "      <td>2020-04-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sherrod Brown</td>\n",
       "      <td>SenSherrodBrown</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Without the #ACA, health insurers could discri...</td>\n",
       "      <td>2020-03-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sherrod Brown</td>\n",
       "      <td>SenSherrodBrown</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Just voted to send more than 15 million in eme...</td>\n",
       "      <td>2020-03-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sherrod Brown</td>\n",
       "      <td>SenSherrodBrown</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Mitch McConnell has wasted four days in the mi...</td>\n",
       "      <td>2020-03-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Sherrod Brown</td>\n",
       "      <td>SenSherrodBrown</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>TUNE IN: At 12:25 PM, I'm going Live with my f...</td>\n",
       "      <td>2020-05-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           name          twitter     party  \\\n",
       "0           0  Sherrod Brown  SenSherrodBrown  Democrat   \n",
       "1           1  Sherrod Brown  SenSherrodBrown  Democrat   \n",
       "2           2  Sherrod Brown  SenSherrodBrown  Democrat   \n",
       "3           3  Sherrod Brown  SenSherrodBrown  Democrat   \n",
       "4           4  Sherrod Brown  SenSherrodBrown  Democrat   \n",
       "\n",
       "                                               tweet tweet_published  \n",
       "0  This year, #WorkersMemorialDay is particularly...      2020-04-28  \n",
       "1  Without the #ACA, health insurers could discri...      2020-03-23  \n",
       "2  Just voted to send more than 15 million in eme...      2020-03-05  \n",
       "3  Mitch McConnell has wasted four days in the mi...      2020-03-17  \n",
       "4  TUNE IN: At 12:25 PM, I'm going Live with my f...      2020-05-21  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21234\n",
      "42820\n"
     ]
    }
   ],
   "source": [
    "dem_tweets = [x for i,x in df.iterrows() if x[\"party\"] == \"Democrat\"]\n",
    "rep_tweets = [x for i,x in df.iterrows() if x[\"party\"] == \"Republican\"]\n",
    "print(len(rep_tweets))\n",
    "print(len(dem_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in c:\\users\\quynh\\anaconda3\\lib\\site-packages (0.5.10)\n",
      "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from num2words) (0.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\quynh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\quynh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\quynh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.party != 'Independent']\n",
    "democrat_tweets_df = df[df.party == 'Democrat']\n",
    "republican_tweets_df = df[df.party == 'Republican']\n",
    "democrat_tweets_df = democrat_tweets_df.sample(frac = 0.5)\n",
    "# df = democrat_tweets_df + republican_tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "republican_tweets_df.drop(['Unnamed: 0'],  axis=1, inplace=True)\n",
    "democrat_tweets_df.drop(['Unnamed: 0'],  axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [democrat_tweets_df, republican_tweets_df]\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Democrat': 21410, 'Republican': 21234})\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "def plot_count(data):\n",
    "    y = Counter(data)\n",
    "    print(y)\n",
    "    sns.countplot(data)\n",
    "plot_count(df['party'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df['tweet'], df['party'], test_size = 0.25, random_state = 42, stratify=df['party'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing some work with feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label={'Democrat':0,'Republican':1}\n",
    "reverse_class_label={0:'Democrat',1:'Republican'} \n",
    "def table_creation(X_train,Y_train):\n",
    "  table={}             # will contain the article count of a term in a class\n",
    "  term_count_in_class={}         # will contain the term count in a class\n",
    "  class_doc_count=np.array([0,0])  # number of articles in a class\n",
    "  class_word_count=np.array([0,0]) # number of words in a class\n",
    "  class_count=np.array([0,0])\n",
    "\n",
    "  for article,label in zip(X_train,Y_train):\n",
    "    #print(article)\n",
    "    class_doc_count[class_label[label]]+=1\n",
    "    class_word_count[class_label[label]]+=len(article)\n",
    "    unique_tokens={-1}\n",
    "    for term in article.split():\n",
    "      #print(term)\n",
    "      #term count in of each term in each class\n",
    "      if term not in term_count_in_class:\n",
    "        term_count_in_class[term]=class_count.copy()\n",
    "      term_count_in_class[term][class_label[label]]+=1\n",
    "      unique_tokens.add(term)\n",
    "    \n",
    "    #print(\"unique tokens in article are\",unique_tokens)\n",
    "    unique_tokens.remove(-1) \n",
    "\n",
    "    for term in unique_tokens:\n",
    "      if term not in table:\n",
    "        table[term]=class_count.copy()\n",
    "      table[term][class_label[label]]+=1\n",
    "  \n",
    "  return table, class_word_count, class_doc_count, term_count_in_class\n",
    "table, class_word_count, class_doc_count, term_count_in_class=table_creation(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "table, class_word_count, class_doc_count, term_count_in_class=create_info(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of articles are 31983.0\n"
     ]
    }
   ],
   "source": [
    "#to calculate mutual information for each term in each class\n",
    "def cal_mutual_info(table,class_word_count,class_doc_count):\n",
    "  N=0\n",
    "  for i in class_doc_count:\n",
    "    N+=i\n",
    "  print(\"total number of articles are\",N)\n",
    "  mi_table=[[],[],[],[]]  # five list to contain the mutual information for each class \n",
    "  for term in table:\n",
    "      #print(\"calculating mi for \",term,\" \",table[term])\n",
    "      for class_ in range(0,2):\n",
    "          #print(\"for class \",class_)\n",
    "          n11=table[term][class_]         # term is present in class\n",
    "          n10=np.sum(table[term])-n11        # term is present but not in the class\n",
    "          n01=class_doc_count[class_]-n11   # number of docs in class not having term \n",
    "          n00=N-(n01+n10+n11)                  # number of docs neither term nor class\n",
    "          #print(\"n11,n10,n01,n00 \",n11,n10,n01,n00)\n",
    "          \n",
    "          if n11==0:\n",
    "              t1=0\n",
    "          else:\n",
    "              t1=(n11/N) * ((np.log(N)+np.log(n11)) - (np.log(n11+n01) + np.log(n11+n10)))\n",
    "          if n01==0:\n",
    "              t2=0\n",
    "          else:\n",
    "              t2=(n01/N) * ((np.log(N)+np.log(n01)) - (np.log(n01+n00) + np.log(n01+n11)))\n",
    "          if n10==0:\n",
    "              t3=0\n",
    "          else:\n",
    "              t3=(n10/N) * ((np.log(N)+np.log(n10)) - (np.log(n10+n11) + np.log(n10+n00)))\n",
    "          if n00==0:\n",
    "              t4=0\n",
    "          else:\n",
    "              t4=(n00/N) * ((np.log(N)+np.log(n00)) - (np.log(n00+n01) + np.log(n00+n10)))\n",
    "          m=t1+t2+t3+t4\n",
    "          mi_table[class_].append(m)\n",
    "  return mi_table\n",
    "mi_table=cal_mutual_info(table,class_word_count, class_doc_count)\n",
    "\n",
    "\n",
    "\n",
    "#creating word map, each word is assigned a uniuqe id like we did for each document\n",
    "def create_word_map(list_of_words):\n",
    "    word_forward_map={}\n",
    "    word_reverse_map={}\n",
    "    count=0\n",
    "    for word in list_of_words:\n",
    "        word_forward_map[word] = count\n",
    "        count = count + 1\n",
    "    word_reverse_map = {v: k for k, v in word_forward_map.items()}\n",
    "    return word_forward_map,word_reverse_map\n",
    "word_forward_map,word_reverse_map=create_word_map(table.keys())\n",
    "\n",
    "\n",
    "\n",
    "#this method will select feaures for each class and returns new vocabulary\n",
    "def feature_selection(mi_table,k,word_forward_map,word_reverse_map):\n",
    "    top_k_words=[]\n",
    "    for class_id in range(0,2):\n",
    "        temp = np.argsort(np.array(mi_table[class_id]))\n",
    "        temp = temp[::-1]\n",
    "        top_k_words.append(temp[:k].copy())\n",
    "    \n",
    "    new_vocab={-1}\n",
    "    count=0\n",
    "    for list_of_words in top_k_words:\n",
    "        #print(list_of_words)\n",
    "        #print(\"for class \",count,\" top \",k,\" words are\")\n",
    "        for wordid in list_of_words:\n",
    "            #print(word_reverse_map[wordid])\n",
    "            new_vocab.add(word_reverse_map[wordid])\n",
    "        count=count+1\n",
    "        \n",
    "    new_vocab.remove(-1)\n",
    "    #print(new_vocab)\n",
    "    return top_k_words,new_vocab\n",
    "\n",
    "top_k_words,new_vocab=feature_selection(mi_table,200,word_forward_map,word_reverse_map)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#to calculate term and class probability \n",
    "def calculate_probability(new_vocab,term_count_in_class,class_word_count,class_doc_count):\n",
    "    term_probability={}\n",
    "    beta=len(new_vocab)\n",
    "    for word in new_vocab:\n",
    "      #print(word)\n",
    "      term_probability[word]=[]\n",
    "      for class_id in range(0,2):\n",
    "        tot=class_word_count[class_id]\n",
    "        #print(\"word count in class\",tot)\n",
    "        tc=term_count_in_class[word][class_id]\n",
    "        p=(tc+1)/(tot+beta)  #add one smoothening\n",
    "        term_probability[word].append(p)    \n",
    "    class_probability=[]\n",
    "    N=np.sum(class_doc_count)\n",
    "    #print(N)\n",
    "    for doc_count in class_doc_count:\n",
    "        class_probability.append(doc_count/N)\n",
    "    return class_probability,term_probability\n",
    "class_probability,term_probability=calculate_probability(new_vocab,term_count_in_class,class_word_count,class_doc_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[3584 1769]\n",
      " [3267 2041]]\n",
      "Accuracy Score : 0.5276240502767096\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Democrat       0.52      0.67      0.59      5353\n",
      "  Republican       0.54      0.38      0.45      5308\n",
      "\n",
      "    accuracy                           0.53     10661\n",
      "   macro avg       0.53      0.53      0.52     10661\n",
      "weighted avg       0.53      0.53      0.52     10661\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5276240502767096"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naive_bayes(new_vocab,term_probability,class_probability,X_test):\n",
    "    y_predicted=[]\n",
    "    for article in X_test:\n",
    "        max_score=-math.inf\n",
    "        score=0\n",
    "        result_class=-1                          #initially document belonging to none class\n",
    "        for class_id in range(0,2):\n",
    "            score=np.log(class_probability[class_id])\n",
    "            for word in article.split():\n",
    "                if word in new_vocab:\n",
    "                    score=score+np.log(term_probability[word][class_id])\n",
    "            #print(\"docid \",docid,\" class \",class_id,\" score \",score)\n",
    "            if score>max_score:\n",
    "                max_score=score\n",
    "                result_class=class_id\n",
    "        #result[docid]=result_class\n",
    "        y_predicted.append(reverse_class_label[result_class])       \n",
    "    return y_predicted          \n",
    "\n",
    "y_predicted=naive_bayes(new_vocab,term_probability,class_probability,X_test)\n",
    "def performance(y_actual,y_predicted):\n",
    "    results = confusion_matrix(y_actual, y_predicted)   \n",
    "    print ('Confusion Matrix :')\n",
    "    print(results) \n",
    "    print ('Accuracy Score :',accuracy_score(y_actual, y_predicted) )\n",
    "    print ('Report : ')\n",
    "    print (classification_report(y_actual, y_predicted) )\n",
    "    return accuracy_score(y_actual, y_predicted)\n",
    "\n",
    "performance(Y_test,y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing Naive Bayes without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8235731846905489\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Democrat       0.81      0.84      0.83      5353\n",
      "  Republican       0.84      0.80      0.82      5308\n",
      "\n",
      "    accuracy                           0.82     10661\n",
      "   macro avg       0.82      0.82      0.82     10661\n",
      "weighted avg       0.82      0.82      0.82     10661\n",
      "\n",
      "0.8236563174186287\n"
     ]
    }
   ],
   "source": [
    "######NaiiveBayes without Feature Selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "Count=CountVectorizer()\n",
    "X_train_count=Count.fit_transform(X_train)\n",
    "X_test_count=Count.transform(X_test)\n",
    "\n",
    "Tf_idf=TfidfTransformer()\n",
    "X_train_tfidf=Tf_idf.fit_transform(X_train_count)\n",
    "X_test_tfidf=Tf_idf.transform(X_test_count)\n",
    "Classifier = MultinomialNB().fit(X_train_tfidf, Y_train)\n",
    "predicted = Classifier.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "print(f1_score(Y_test, predicted,average='weighted'))\n",
    "print(classification_report(Y_test,predicted))\n",
    "print(accuracy_score(Y_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing SVM with Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM without feature Selection using different approaches:\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC \n",
    "# SVM with feature selection\n",
    "def produce_count_vector(X_data,vocab,term_count_in_class):\n",
    "    data_count=[]\n",
    "    for index in X_data:\n",
    "        count_vector=[]\n",
    "        values=index.split(' ')\n",
    "        for word in vocab:\n",
    "            if word in values:\n",
    "                count_vector.append(values.count(word))\n",
    "            else:\n",
    "                count_vector.append(0)\n",
    "        data_count.append(count_vector)\n",
    "    return data_count\n",
    "\n",
    "X_train_count=produce_count_vector(X_train,new_vocab,term_count_in_class)\n",
    "X_test_count=produce_count_vector(X_test,new_vocab,term_count_in_class)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "vector=TfidfTransformer()\n",
    "X_train_vector=vector.fit_transform(X_train_count)\n",
    "X_test_vector=vector.transform(X_test_count)\n",
    "classifier=SVC(verbose=True).fit(X_train_vector,Y_train)\n",
    "predicted=classifier.predict(X_test_vector)\n",
    "print(f1_score(Y_test,predicted,average='weighted'))\n",
    "print(classification_report(Y_test,predicted))\n",
    "print(accuracy_score(Y_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For unigrams--------------------\n",
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "# SVM with tf-idf\n",
    "#1. tf-idf using n grams\n",
    "#a) unigram\n",
    "print(\"For unigrams--------------------\")\n",
    "vector=TfidfVectorizer(analyzer='word',ngram_range=(1,1))\n",
    "X_train_vector=vector.fit_transform(X_train)\n",
    "X_test_vector=vector.transform(X_test)\n",
    "classifier=SVC(verbose=True).fit(X_train_vector,Y_train)\n",
    "predicted=classifier.predict(X_test_vector)\n",
    "print(f1_score(Y_test,predicted,average='weighted'))\n",
    "print(classification_report(Y_test,predicted))\n",
    "print(accuracy_score(Y_test,predicted))\n",
    "\n",
    "\n",
    "print(\"For Bigrams----------------------\")\n",
    "#b) bigram\n",
    "vector=TfidfVectorizer(analyzer='word',ngram_range=(2,2))\n",
    "X_train_vector=vector.fit_transform(X_train)\n",
    "X_test_vector=vector.transform(X_test)\n",
    "classifier=SVC(verbose=True).fit(X_train_vector,Y_train)\n",
    "predicted=classifier.predict(X_test_vector)\n",
    "print(f1_score(Y_test,predicted,average='weighted'))\n",
    "print(classification_report(Y_test,predicted))\n",
    "print(accuracy_score(Y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
