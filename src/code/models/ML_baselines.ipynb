{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>twitter</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sherrod Brown</td>\n",
       "      <td>SenSherrodBrown</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maria Cantwell</td>\n",
       "      <td>SenatorCantwell</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Benjamin L. Cardin</td>\n",
       "      <td>SenatorCardin</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thomas R. Carper</td>\n",
       "      <td>SenatorCarper</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robert P. Casey, Jr.</td>\n",
       "      <td>SenBobCasey</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name          twitter     party\n",
       "0         Sherrod Brown  SenSherrodBrown  Democrat\n",
       "1        Maria Cantwell  SenatorCantwell  Democrat\n",
       "2    Benjamin L. Cardin    SenatorCardin  Democrat\n",
       "3      Thomas R. Carper    SenatorCarper  Democrat\n",
       "4  Robert P. Casey, Jr.      SenBobCasey  Democrat"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame, read_csv\n",
    "import pandas as pd \n",
    "twitter_url_reps = r'reps.csv'\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "acc_df = pd.read_csv(twitter_url_reps)\n",
    "df2 = acc_df[['name', 'twitter', 'party']]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./tweets/tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21234\n",
      "42820\n"
     ]
    }
   ],
   "source": [
    "dem_tweets = [x for i,x in df.iterrows() if x[\"party\"] == \"Democrat\"]\n",
    "rep_tweets = [x for i,x in df.iterrows() if x[\"party\"] == \"Republican\"]\n",
    "print(len(rep_tweets))\n",
    "print(len(dem_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting num2words\n",
      "  Using cached num2words-0.5.10-py3-none-any.whl (101 kB)\n",
      "Collecting docopt>=0.6.2\n",
      "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13709 sha256=8bbd98d9d1e121c0249b26f0db157fe3a6602dc88fd08305e46b502502927ed8\n",
      "  Stored in directory: c:\\users\\quynh\\appdata\\local\\pip\\cache\\wheels\\3f\\2a\\fa\\4d7a888e69774d5e6e855d190a8a51b357d77cc05eb1c097c9\n",
      "Successfully built docopt\n",
      "Installing collected packages: docopt, num2words\n",
      "Successfully installed docopt-0.6.2 num2words-0.5.10\n"
     ]
    }
   ],
   "source": [
    "!pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.party != 'Independent']\n",
    "democrat_tweets_df = df[df.party == 'Democrat']\n",
    "republican_tweets_df = df[df.party == 'Republican']\n",
    "democrat_tweets_df = democrat_tweets_df.sample(frac = 0.5)\n",
    "# df = democrat_tweets_df + republican_tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "republican_tweets_df.drop(['Unnamed: 0'],  axis=1, inplace=True)\n",
    "democrat_tweets_df.drop(['Unnamed: 0'],  axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [democrat_tweets_df, republican_tweets_df]\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\quynh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\quynh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\quynh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Democrat': 21410, 'Republican': 21234})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quynh\\anaconda3\\envs\\tim\\lib\\site-packages\\seaborn\\_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "def plot_count(data):\n",
    "    y = Counter(data)\n",
    "    print(y)\n",
    "    sns.countplot(data)\n",
    "plot_count(df['party'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df['tweet'], df['party'], test_size = 0.25, random_state = 42, stratify=df['party'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "random.seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tensorflow._api.v2.version' from 'C:\\\\Users\\\\quynh\\\\anaconda3\\\\envs\\\\tim\\\\lib\\\\site-packages\\\\tensorflow\\\\_api\\\\v2\\\\version\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.version)\n",
    "\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GRU, LSTM\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn import tree\n",
    "import copy\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras.layers import Dropout, Dense, GRU, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dropout, Dense,Input,Embedding,Flatten, AveragePooling2D, Conv2D,Reshape\n",
    "from tensorflow.keras.models import Sequential,Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(X_train, y_train, X_test, y_test):\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LinearSVC()),\n",
    "                     ])\n",
    "\n",
    "    text_clf.fit(X_train, y_train)\n",
    "    # Save the model here (uncomment the line below)\n",
    "    pkl.dump(text_clf, open('./results/svm.pkl', 'wb'))\n",
    "\n",
    "\n",
    "    predicted = text_clf.predict(X_test)\n",
    "\n",
    "    print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Democrat       0.84      0.83      0.83      5353\n",
      "  Republican       0.83      0.84      0.84      5308\n",
      "\n",
      "    accuracy                           0.83     10661\n",
      "   macro avg       0.83      0.83      0.83     10661\n",
      "weighted avg       0.83      0.83      0.83     10661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "svm_results = pickle.load(open(\"./results/svm.pkl\", \"rb\"))\n",
    "svm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Democrat       0.71      0.70      0.71      5353\n",
      "  Republican       0.70      0.72      0.71      5308\n",
      "\n",
      "    accuracy                           0.71     10661\n",
      "   macro avg       0.71      0.71      0.71     10661\n",
      "weighted avg       0.71      0.71      0.71     10661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def rocchio(X_train, y_train, X_test, y_test):\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', NearestCentroid()),\n",
    "                         ])\n",
    "\n",
    "    text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    predicted = text_clf.predict(X_test)\n",
    "\n",
    "    print(metrics.classification_report(y_test, predicted))\n",
    "rocchio(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train, X_test, y_test):\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', RandomForestClassifier(n_estimators=100)),\n",
    "                         ])\n",
    "\n",
    "    text_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Save the model here (uncomment the line below)\n",
    "    # pkl.dump(text_clf, open('./relevance-model.pkl', 'wb'))\n",
    "\n",
    "    predicted = text_clf.predict(X_test)\n",
    "\n",
    "    print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Democrat       0.77      0.80      0.78      5353\n",
      "  Republican       0.79      0.75      0.77      5308\n",
      "\n",
      "    accuracy                           0.78     10661\n",
      "   macro avg       0.78      0.78      0.78     10661\n",
      "weighted avg       0.78      0.78      0.78     10661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(X_train, y_train, X_test, y_test):\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', MultinomialNB()),\n",
    "                         ])\n",
    "\n",
    "    text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    predicted = text_clf.predict(X_test)\n",
    "\n",
    "    print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Democrat       0.81      0.83      0.82      5353\n",
      "  Republican       0.82      0.80      0.81      5308\n",
      "\n",
      "    accuracy                           0.82     10661\n",
      "   macro avg       0.82      0.82      0.82     10661\n",
      "weighted avg       0.82      0.82      0.82     10661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "naive_bayes(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train, y_train, X_test, y_test):\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', KNeighborsClassifier()),\n",
    "                         ])\n",
    "\n",
    "    text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    predicted = text_clf.predict(X_test)\n",
    "\n",
    "    print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Democrat       0.78      0.77      0.77      5353\n",
      "  Republican       0.77      0.78      0.78      5308\n",
      "\n",
      "    accuracy                           0.78     10661\n",
      "   macro avg       0.78      0.78      0.78     10661\n",
      "weighted avg       0.78      0.78      0.78     10661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_trees(X_train, y_train, X_test, y_test):\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', tree.DecisionTreeClassifier()),\n",
    "                         ])\n",
    "\n",
    "    text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    predicted = text_clf.predict(X_test)\n",
    "\n",
    "    print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Democrat       0.65      0.66      0.66      5353\n",
      "  Republican       0.65      0.65      0.65      5308\n",
      "\n",
      "    accuracy                           0.65     10661\n",
      "   macro avg       0.65      0.65      0.65     10661\n",
      "weighted avg       0.65      0.65      0.65     10661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decision_trees(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 Solution\n",
    "\n",
    "# --- Insert code here ---\n",
    "class AdaBoostTrees(object):\n",
    "    \n",
    "#   Note: You will need to remove the \"pass\" statements\n",
    "\n",
    "    def __init__(self, n_estimators=10, max_depth=1):\n",
    "\n",
    "#       self.n_estimators : The number of decision tree classifiers used in the ensemble.\n",
    "#       self.max_depth    : The max_depth setting for each of the DecisionTree objects.\n",
    "#       self.trees        : A list containing all of the DecisionTree objects.\n",
    "#       self.tree_weights : A list containing the weight of each of the trained DecisionTree objects.\n",
    "\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.tree_weights = []\n",
    "    \n",
    "    def fit(self, x_train, y_train):\n",
    "        \n",
    "#       This method takes as input two numpy arrays, x_train and y_train,\n",
    "#       and modifies the AdaBoostTrees object in-place. This method implements\n",
    "#       the AdaBoost algorithm using sci-kit learn's DecisionTreeClassifier\n",
    "#       as the \"weak learner\".\n",
    "\n",
    "#       --- Insert code here ---\n",
    "        # Samples x1.... xN\n",
    "        num_samples = len(y_train)\n",
    "        \n",
    "        # Initialize the observation weights to 1 / N\n",
    "        wt = np.ones(num_samples) / num_samples\n",
    "        \n",
    "        # Number of weak classifiers used\n",
    "        M = self.n_estimators\n",
    "              \n",
    "        for i in range(M):\n",
    "            # 1. Fit a classifier G(x) to the training data using weights Wi\n",
    "            tree_estimator = tree.DecisionTreeClassifier(max_depth = self.max_depth)\n",
    "            tree_estimator.fit(x_train, y_train, sample_weight = wt)\n",
    "            predict = tree_estimator.predict(x_train)\n",
    "            \n",
    "            # Get incorrect classifications\n",
    "            incorrect = [int(x) for x in predict != y_train]\n",
    "            \n",
    "            # Find the weak learner that minimizes the weighted \n",
    "            # sum error for misclassified points\n",
    "            err = np.dot(wt, incorrect) / sum(wt)\n",
    "            \n",
    "            # Compute alpha, which is the say - maybe add constant term if \n",
    "            # freaks out\n",
    "            alpha = 0.5 * np.log((1 - err) / err)\n",
    "            \n",
    "            # Update the weights\n",
    "            wt = np.multiply(wt, np.exp([alpha * i for i in incorrect]))\n",
    "            \n",
    "            # Normalize to 1\n",
    "            wt /= np.sum(wt)\n",
    "            \n",
    "            # Possibly add it to the list of tree_weights\n",
    "            self.tree_weights.append(wt)\n",
    "            \n",
    "            # Add the classifier\n",
    "            self.trees.append((tree_estimator, alpha))\n",
    "            \n",
    "    def predict(self, x):\n",
    "        \n",
    "#       This method takes as input a numpy array of size (N, D) and \n",
    "#       returns a numpy array of predictions of size (N). Calling this \n",
    "#       method requires that fit() has already been run on the training data.    \n",
    "\n",
    "#       --- Insert code here ---        \n",
    "        pred_test = np.zeros(len(x))\n",
    "        \n",
    "        for clf, alpha in self.trees:\n",
    "            # 1. Fit a classifier G(x) to the training data using weights Wi\n",
    "            pred_test_i = clf.predict(x)\n",
    "            \n",
    "            pred_test = [sum(x) for x in zip(pred_test, \n",
    "                                         [x * alpha for x in pred_test_i])]\n",
    "\n",
    "        # Return sign of prediction sum\n",
    "        y_pred = np.sign(pred_test)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-666729bd06f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n\u001b[0m\u001b[0;32m     10\u001b[0m                                            \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                            shuffle=True)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Make the dataset iterable\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(X_test) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
