{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in c:\\users\\quynh\\anaconda3\\lib\\site-packages (0.5.10)\n",
      "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from num2words) (0.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install num2words\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('covid_tweets.csv')\n",
    "democrat_tweets_df = df[df.party == 'Democrat']\n",
    "republican_tweets_df = df[df.party == 'Republican']\n",
    "democrat_tweets_df = democrat_tweets_df.sample(frac = 0.5)\n",
    "frames = [democrat_tweets_df, republican_tweets_df]\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>twitter</th>\n",
       "      <th>party</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34724</th>\n",
       "      <td>Greg Stanton</td>\n",
       "      <td>RepGregStanton</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Thank you, @dougducey, this is the right thing...</td>\n",
       "      <td>2020-04-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14973</th>\n",
       "      <td>Lucille Roybal-Allard</td>\n",
       "      <td>RepRoybalAllard</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>As we all confront the threat of the #coronavi...</td>\n",
       "      <td>2020-03-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34430</th>\n",
       "      <td>Ann Kirkpatrick</td>\n",
       "      <td>RepKirkpatrick</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>I understood the challenges that local newspap...</td>\n",
       "      <td>2020-07-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21239</th>\n",
       "      <td>Alan S. Lowenthal</td>\n",
       "      <td>RepLowenthal</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>If you have any #COVID19 symptoms, stay home. ...</td>\n",
       "      <td>2020-03-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16774</th>\n",
       "      <td>Terri A. Sewell</td>\n",
       "      <td>RepTerriSewell</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Please take a moment to sign up for free text ...</td>\n",
       "      <td>2020-03-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name          twitter     party  \\\n",
       "34724           Greg Stanton   RepGregStanton  Democrat   \n",
       "14973  Lucille Roybal-Allard  RepRoybalAllard  Democrat   \n",
       "34430        Ann Kirkpatrick   RepKirkpatrick  Democrat   \n",
       "21239      Alan S. Lowenthal     RepLowenthal  Democrat   \n",
       "16774        Terri A. Sewell   RepTerriSewell  Democrat   \n",
       "\n",
       "                                                   tweet tweet_published  \n",
       "34724  Thank you, @dougducey, this is the right thing...      2020-04-06  \n",
       "14973  As we all confront the threat of the #coronavi...      2020-03-13  \n",
       "34430  I understood the challenges that local newspap...      2020-07-28  \n",
       "21239  If you have any #COVID19 symptoms, stay home. ...      2020-03-15  \n",
       "16774  Please take a moment to sign up for free text ...      2020-03-16  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None )\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "covid_stop_words = {\"covid\", \"covid19\", \"covid 19\", \"corona\", \"coronavirus\"}\n",
    "porter = PorterStemmer()\n",
    "def preprocess_and_tokenize(tweet):\n",
    "    # Remove non-letters, lowercase everything, remove stop words, and stem\n",
    "    lower_letters = re.sub(r'[^A-Za-z0-9 ]+', \" \", tweet).lower().split()\n",
    "    important_words = []\n",
    "    for w in lower_letters:\n",
    "        if w not in stop_words and w not in covid_stop_words:\n",
    "            important_words.append(w)\n",
    "    return important_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenize_tweet'] = df['tweet'].apply(lambda x : preprocess_and_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "dems, reps = list(), list()\n",
    "for l in df.party:\n",
    "    if l == 'Democrat':\n",
    "        dems.append(1)\n",
    "        reps.append(0)\n",
    "    elif l == 'Republican':\n",
    "        reps.append(1)\n",
    "        dems.append(0)\n",
    "df['dems']= dems\n",
    "df['reps']= reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>twitter</th>\n",
       "      <th>party</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_published</th>\n",
       "      <th>tokenize_tweet</th>\n",
       "      <th>dems</th>\n",
       "      <th>reps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34724</th>\n",
       "      <td>Greg Stanton</td>\n",
       "      <td>RepGregStanton</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Thank you, @dougducey, this is the right thing...</td>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>[thank, dougducey, right, thing, small, busine...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14973</th>\n",
       "      <td>Lucille Roybal-Allard</td>\n",
       "      <td>RepRoybalAllard</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>As we all confront the threat of the #coronavi...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>[confront, threat, please, read, newsletter, d...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34430</th>\n",
       "      <td>Ann Kirkpatrick</td>\n",
       "      <td>RepKirkpatrick</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>I understood the challenges that local newspap...</td>\n",
       "      <td>2020-07-28</td>\n",
       "      <td>[understood, challenges, local, newspapers, fa...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21239</th>\n",
       "      <td>Alan S. Lowenthal</td>\n",
       "      <td>RepLowenthal</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>If you have any #COVID19 symptoms, stay home. ...</td>\n",
       "      <td>2020-03-15</td>\n",
       "      <td>[symptoms, stay, home, voted, ensure, american...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16774</th>\n",
       "      <td>Terri A. Sewell</td>\n",
       "      <td>RepTerriSewell</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Please take a moment to sign up for free text ...</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>[please, take, moment, sign, free, text, updat...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name          twitter     party  \\\n",
       "34724           Greg Stanton   RepGregStanton  Democrat   \n",
       "14973  Lucille Roybal-Allard  RepRoybalAllard  Democrat   \n",
       "34430        Ann Kirkpatrick   RepKirkpatrick  Democrat   \n",
       "21239      Alan S. Lowenthal     RepLowenthal  Democrat   \n",
       "16774        Terri A. Sewell   RepTerriSewell  Democrat   \n",
       "\n",
       "                                                   tweet tweet_published  \\\n",
       "34724  Thank you, @dougducey, this is the right thing...      2020-04-06   \n",
       "14973  As we all confront the threat of the #coronavi...      2020-03-13   \n",
       "34430  I understood the challenges that local newspap...      2020-07-28   \n",
       "21239  If you have any #COVID19 symptoms, stay home. ...      2020-03-15   \n",
       "16774  Please take a moment to sign up for free text ...      2020-03-16   \n",
       "\n",
       "                                          tokenize_tweet  dems  reps  \n",
       "34724  [thank, dougducey, right, thing, small, busine...     1     0  \n",
       "14973  [confront, threat, please, read, newsletter, d...     1     0  \n",
       "34430  [understood, challenges, local, newspapers, fa...     1     0  \n",
       "21239  [symptoms, stay, home, voted, ensure, american...     1     0  \n",
       "16774  [please, take, moment, sign, free, text, updat...     1     0  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test = train_test_split(df, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We build training vocab/max trailing sentence length/total number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "694371 words total, with a vocabulary size of 32817\n",
      "Max sentence length is 91\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in data_train[\"tokenize_tweet\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokenize_tweet\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build testing vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232213 words total, with a vocabulary size of 19069\n",
      "Max sentence length is 77\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in data_test[\"tokenize_tweet\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokenize_tweet\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quynh\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "### Load up word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "word2vec.wv.save_word2vec_format('googlenews.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize & Pad the Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36680 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"tweet\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"tweet\"].tolist())\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "train_rnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will get embeddings from Google News Word2Vec model and save them corresponding to the sequence number we assigned to each word. If we could not get embeddings we save a random vector for that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36681, 300)\n"
     ]
    }
   ],
   "source": [
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"tweet\"].tolist())\n",
    "test_rnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['dems', 'reps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train[label_names].values\n",
    "x_train = train_rnn_data\n",
    "y_tr = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[]\n",
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.config.list_physical_devices('GPU')) \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurrent_nn(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    lstm = LSTM(256)(embedded_sequences)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(lstm)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None )\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_13 (Embedding)     (None, 50, 300)           11004300  \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 256)               570368    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 11,607,822\n",
      "Trainable params: 603,522\n",
      "Non-trainable params: 11,004,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model = recurrent_nn(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM,  len(list(label_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1440/1440 [==============================] - 117s 81ms/step - loss: 0.6275 - acc: 0.6444 - val_loss: 0.5817 - val_acc: 0.6987\n",
      "Epoch 2/5\n",
      "1440/1440 [==============================] - 111s 77ms/step - loss: 0.5470 - acc: 0.7148 - val_loss: 0.5155 - val_acc: 0.7324\n",
      "Epoch 3/5\n",
      "1440/1440 [==============================] - 110s 77ms/step - loss: 0.5006 - acc: 0.7465 - val_loss: 0.4901 - val_acc: 0.7599\n",
      "Epoch 4/5\n",
      "1440/1440 [==============================] - 114s 79ms/step - loss: 0.4554 - acc: 0.7776 - val_loss: 0.4824 - val_acc: 0.7702\n",
      "Epoch 5/5\n",
      "1440/1440 [==============================] - 114s 79ms/step - loss: 0.3980 - acc: 0.8097 - val_loss: 0.5091 - val_acc: 0.7702\n"
     ]
    }
   ],
   "source": [
    "hist = rnn_model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.10, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 5s 243ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = rnn_model.predict(test_rnn_data, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7585592345933777"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [1, 0]\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "sum(data_test['dems'] == prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a CNN\n",
    "# 1. Embeddings matrix passed to embedding layer\n",
    "# 2. Five different filter sizese are applied to each tweet\n",
    "# 3. GlobalMaxPooling1D layers are applied to each layer\n",
    "# 4. All outputs concatenated\n",
    "# 5. Dropout -> Dense -> Dropout -> Dense applied\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(embeddings, max_seq_length, num_words, \n",
    "        embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                               embedding_dim,\n",
    "                               weights=[embeddings],\n",
    "                               input_length=max_seq_length,\n",
    "                               trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "    \n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, \n",
    "                        kernel_size=filter_size, \n",
    "                        activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "        \n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "    \n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_23\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 50, 300)      11004300    input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 49, 200)      120200      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 48, 200)      180200      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 47, 200)      240200      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 46, 200)      300200      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 45, 200)      360200      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 200)          0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 200)          0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 200)          0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 200)          0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 200)          0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1000)         0           global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "                                                                 global_max_pooling1d_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 1000)         0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 128)          128128      dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 128)          0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 2)            258         dropout_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 12,333,686\n",
      "Trainable params: 1,329,386\n",
      "Non-trainable params: 11,004,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300\n",
    "cnn_model = cnn(train_embedding_weights, \n",
    "                MAX_SEQUENCE_LENGTH, \n",
    "                len(train_word_index)+1, \n",
    "                EMBEDDING_DIM, \n",
    "                len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1440/1440 [==============================] - 49s 34ms/step - loss: 0.6433 - acc: 0.6128 - val_loss: 0.5361 - val_acc: 0.7284\n",
      "Epoch 2/5\n",
      "1440/1440 [==============================] - 48s 33ms/step - loss: 0.5344 - acc: 0.7274 - val_loss: 0.5173 - val_acc: 0.7543\n",
      "Epoch 3/5\n",
      "1440/1440 [==============================] - 50s 35ms/step - loss: 0.4441 - acc: 0.7972 - val_loss: 0.4473 - val_acc: 0.7837\n",
      "Epoch 4/5\n",
      "1440/1440 [==============================] - 49s 34ms/step - loss: 0.3426 - acc: 0.8539 - val_loss: 0.4538 - val_acc: 0.7934\n",
      "Epoch 5/5\n",
      "1440/1440 [==============================] - 48s 33ms/step - loss: 0.2418 - acc: 0.9034 - val_loss: 0.4919 - val_acc: 0.7962\n"
     ]
    }
   ],
   "source": [
    "hist = cnn_model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.10, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 4s 348ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = cnn_model.predict(test_rnn_data, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7779757996435607"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [1, 0]\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "sum(data_test['dems'] == prediction_labels)/len(prediction_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
