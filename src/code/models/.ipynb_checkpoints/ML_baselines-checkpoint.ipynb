{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>twitter</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sherrod Brown</td>\n",
       "      <td>SenSherrodBrown</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maria Cantwell</td>\n",
       "      <td>SenatorCantwell</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Benjamin L. Cardin</td>\n",
       "      <td>SenatorCardin</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thomas R. Carper</td>\n",
       "      <td>SenatorCarper</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robert P. Casey, Jr.</td>\n",
       "      <td>SenBobCasey</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name          twitter     party\n",
       "0         Sherrod Brown  SenSherrodBrown  Democrat\n",
       "1        Maria Cantwell  SenatorCantwell  Democrat\n",
       "2    Benjamin L. Cardin    SenatorCardin  Democrat\n",
       "3      Thomas R. Carper    SenatorCarper  Democrat\n",
       "4  Robert P. Casey, Jr.      SenBobCasey  Democrat"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame, read_csv\n",
    "import pandas as pd \n",
    "twitter_url_reps = r'reps.csv'\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "acc_df = pd.read_csv(twitter_url_reps)\n",
    "df2 = acc_df[['name', 'twitter', 'party']]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./tweets/tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21234\n",
      "42820\n"
     ]
    }
   ],
   "source": [
    "dem_tweets = [x for i,x in df.iterrows() if x[\"party\"] == \"Democrat\"]\n",
    "rep_tweets = [x for i,x in df.iterrows() if x[\"party\"] == \"Republican\"]\n",
    "print(len(rep_tweets))\n",
    "print(len(dem_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting num2words\n",
      "  Using cached num2words-0.5.10-py3-none-any.whl (101 kB)\n",
      "Collecting docopt>=0.6.2\n",
      "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13709 sha256=8bbd98d9d1e121c0249b26f0db157fe3a6602dc88fd08305e46b502502927ed8\n",
      "  Stored in directory: c:\\users\\quynh\\appdata\\local\\pip\\cache\\wheels\\3f\\2a\\fa\\4d7a888e69774d5e6e855d190a8a51b357d77cc05eb1c097c9\n",
      "Successfully built docopt\n",
      "Installing collected packages: docopt, num2words\n",
      "Successfully installed docopt-0.6.2 num2words-0.5.10\n"
     ]
    }
   ],
   "source": [
    "!pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.party != 'Independent']\n",
    "democrat_tweets_df = df[df.party == 'Democrat']\n",
    "republican_tweets_df = df[df.party == 'Republican']\n",
    "democrat_tweets_df = democrat_tweets_df.sample(frac = 0.5)\n",
    "# df = democrat_tweets_df + republican_tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "republican_tweets_df.drop(['Unnamed: 0'],  axis=1, inplace=True)\n",
    "democrat_tweets_df.drop(['Unnamed: 0'],  axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [democrat_tweets_df, republican_tweets_df]\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\quynh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\quynh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\quynh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Democrat': 21410, 'Republican': 21234})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quynh\\anaconda3\\envs\\tim\\lib\\site-packages\\seaborn\\_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "def plot_count(data):\n",
    "    y = Counter(data)\n",
    "    print(y)\n",
    "    sns.countplot(data)\n",
    "plot_count(df['party'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df['tweet'], df['party'], test_size = 0.25, random_state = 42, stratify=df['party'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "random.seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tensorflow._api.v2.version' from 'C:\\\\Users\\\\quynh\\\\anaconda3\\\\envs\\\\tim\\\\lib\\\\site-packages\\\\tensorflow\\\\_api\\\\v2\\\\version\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.version)\n",
    "\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GRU, LSTM\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn import tree\n",
    "import copy\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras.layers import Dropout, Dense, GRU, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dropout, Dense,Input,Embedding,Flatten, AveragePooling2D, Conv2D,Reshape\n",
    "from tensorflow.keras.models import Sequential,Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(X_train, y_train, X_test, y_test):\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LinearSVC()),\n",
    "                     ])\n",
    "\n",
    "    text_clf.fit(X_train, y_train)\n",
    "    # Save the model here (uncomment the line below)\n",
    "    pkl.dump(text_clf, open('./results/svm.pkl', 'wb'))\n",
    "\n",
    "\n",
    "    predicted = text_clf.predict(X_test)\n",
    "\n",
    "    print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Democrat       0.84      0.83      0.83      5353\n",
      "  Republican       0.83      0.84      0.84      5308\n",
      "\n",
      "    accuracy                           0.83     10661\n",
      "   macro avg       0.83      0.83      0.83     10661\n",
      "weighted avg       0.83      0.83      0.83     10661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "svm_results = pickle.load(open(\"./results/svm.pkl\", \"rb\"))\n",
    "svm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Democrat       0.71      0.70      0.71      5353\n",
      "  Republican       0.70      0.72      0.71      5308\n",
      "\n",
      "    accuracy                           0.71     10661\n",
      "   macro avg       0.71      0.71      0.71     10661\n",
      "weighted avg       0.71      0.71      0.71     10661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def rocchio(X_train, y_train, X_test, y_test):\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', NearestCentroid()),\n",
    "                         ])\n",
    "\n",
    "    text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    predicted = text_clf.predict(X_test)\n",
    "\n",
    "    print(metrics.classification_report(y_test, predicted))\n",
    "rocchio(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train, X_test, y_test):\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', RandomForestClassifier(n_estimators=100)),\n",
    "                         ])\n",
    "\n",
    "    text_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Save the model here (uncomment the line below)\n",
    "    # pkl.dump(text_clf, open('./relevance-model.pkl', 'wb'))\n",
    "\n",
    "    predicted = text_clf.predict(X_test)\n",
    "\n",
    "    print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Democrat       0.77      0.80      0.78      5353\n",
      "  Republican       0.79      0.75      0.77      5308\n",
      "\n",
      "    accuracy                           0.78     10661\n",
      "   macro avg       0.78      0.78      0.78     10661\n",
      "weighted avg       0.78      0.78      0.78     10661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(X_train, y_train, X_test, y_test):\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', MultinomialNB()),\n",
    "                         ])\n",
    "\n",
    "    text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    predicted = text_clf.predict(X_test)\n",
    "\n",
    "    print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Democrat       0.81      0.83      0.82      5353\n",
      "  Republican       0.82      0.80      0.81      5308\n",
      "\n",
      "    accuracy                           0.82     10661\n",
      "   macro avg       0.82      0.82      0.82     10661\n",
      "weighted avg       0.82      0.82      0.82     10661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "naive_bayes(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train, y_train, X_test, y_test):\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', KNeighborsClassifier()),\n",
    "                         ])\n",
    "\n",
    "    text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    predicted = text_clf.predict(X_test)\n",
    "\n",
    "    print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Democrat       0.78      0.77      0.77      5353\n",
      "  Republican       0.77      0.78      0.78      5308\n",
      "\n",
      "    accuracy                           0.78     10661\n",
      "   macro avg       0.78      0.78      0.78     10661\n",
      "weighted avg       0.78      0.78      0.78     10661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_trees(X_train, y_train, X_test, y_test):\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', tree.DecisionTreeClassifier()),\n",
    "                         ])\n",
    "\n",
    "    text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    predicted = text_clf.predict(X_test)\n",
    "\n",
    "    print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Democrat       0.65      0.66      0.66      5353\n",
      "  Republican       0.65      0.65      0.65      5308\n",
      "\n",
      "    accuracy                           0.65     10661\n",
      "   macro avg       0.65      0.65      0.65     10661\n",
      "weighted avg       0.65      0.65      0.65     10661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decision_trees(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 Solution\n",
    "\n",
    "# --- Insert code here ---\n",
    "class AdaBoostTrees(object):\n",
    "    \n",
    "#   Note: You will need to remove the \"pass\" statements\n",
    "\n",
    "    def __init__(self, n_estimators=10, max_depth=1):\n",
    "\n",
    "#       self.n_estimators : The number of decision tree classifiers used in the ensemble.\n",
    "#       self.max_depth    : The max_depth setting for each of the DecisionTree objects.\n",
    "#       self.trees        : A list containing all of the DecisionTree objects.\n",
    "#       self.tree_weights : A list containing the weight of each of the trained DecisionTree objects.\n",
    "\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.tree_weights = []\n",
    "    \n",
    "    def fit(self, x_train, y_train):\n",
    "        \n",
    "#       This method takes as input two numpy arrays, x_train and y_train,\n",
    "#       and modifies the AdaBoostTrees object in-place. This method implements\n",
    "#       the AdaBoost algorithm using sci-kit learn's DecisionTreeClassifier\n",
    "#       as the \"weak learner\".\n",
    "\n",
    "#       --- Insert code here ---\n",
    "        # Samples x1.... xN\n",
    "        num_samples = len(y_train)\n",
    "        \n",
    "        # Initialize the observation weights to 1 / N\n",
    "        wt = np.ones(num_samples) / num_samples\n",
    "        \n",
    "        # Number of weak classifiers used\n",
    "        M = self.n_estimators\n",
    "              \n",
    "        for i in range(M):\n",
    "            # 1. Fit a classifier G(x) to the training data using weights Wi\n",
    "            tree_estimator = tree.DecisionTreeClassifier(max_depth = self.max_depth)\n",
    "            tree_estimator.fit(x_train, y_train, sample_weight = wt)\n",
    "            predict = tree_estimator.predict(x_train)\n",
    "            \n",
    "            # Get incorrect classifications\n",
    "            incorrect = [int(x) for x in predict != y_train]\n",
    "            \n",
    "            # Find the weak learner that minimizes the weighted \n",
    "            # sum error for misclassified points\n",
    "            err = np.dot(wt, incorrect) / sum(wt)\n",
    "            \n",
    "            # Compute alpha, which is the say - maybe add constant term if \n",
    "            # freaks out\n",
    "            alpha = 0.5 * np.log((1 - err) / err)\n",
    "            \n",
    "            # Update the weights\n",
    "            wt = np.multiply(wt, np.exp([alpha * i for i in incorrect]))\n",
    "            \n",
    "            # Normalize to 1\n",
    "            wt /= np.sum(wt)\n",
    "            \n",
    "            # Possibly add it to the list of tree_weights\n",
    "            self.tree_weights.append(wt)\n",
    "            \n",
    "            # Add the classifier\n",
    "            self.trees.append((tree_estimator, alpha))\n",
    "            \n",
    "    def predict(self, x):\n",
    "        \n",
    "#       This method takes as input a numpy array of size (N, D) and \n",
    "#       returns a numpy array of predictions of size (N). Calling this \n",
    "#       method requires that fit() has already been run on the training data.    \n",
    "\n",
    "#       --- Insert code here ---        \n",
    "        pred_test = np.zeros(len(x))\n",
    "        \n",
    "        for clf, alpha in self.trees:\n",
    "            # 1. Fit a classifier G(x) to the training data using weights Wi\n",
    "            pred_test_i = clf.predict(x)\n",
    "            \n",
    "            pred_test = [sum(x) for x in zip(pred_test, \n",
    "                                         [x * alpha for x in pred_test_i])]\n",
    "\n",
    "        # Return sign of prediction sum\n",
    "        y_pred = np.sign(pred_test)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-666729bd06f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n\u001b[0m\u001b[0;32m     10\u001b[0m                                            \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                            shuffle=True)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Make the dataset iterable\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(X_test) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To replace with my own code later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=50):\n",
    "    np.random.seed(7)\n",
    "    text = np.concatenate((X_train, X_test), axis=0)\n",
    "    text = np.array(text)\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    indices = np.arange(text.shape[0])\n",
    "    # np.random.shuffle(indices)\n",
    "    text = text[indices]\n",
    "    print(text.shape)\n",
    "    X_train = text[0:len(X_train), ]\n",
    "    X_test = text[len(X_train):, ]\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    # Download the glOve embeddings - 50 dimensional and use the appropriate path below\n",
    "    f = open(\"./dataset/glove.6B.50d.txt\", encoding=\"utf8\")\n",
    "    for line in f:\n",
    "\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    return (X_train, X_test, word_index,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Model_RNN_Text(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH=50, EMBEDDING_DIM=50, dropout=0.5):\n",
    "    \"\"\"\n",
    "    def buildModel_RNN(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
    "    word_index in word index ,\n",
    "    embeddings_index is embeddings index, look at data_helper.py\n",
    "    nClasses is number of classes,\n",
    "    MAX_SEQUENCE_LENGTH is maximum lenght of text sequences\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    hidden_layer = 3\n",
    "    gru_node = 256\n",
    "\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if len(embedding_matrix[i]) != len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n",
    "                      \"into shape\", str(len(embedding_vector)), \" Please make sure your\"\n",
    "                                                                \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
    "                exit(1)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True))\n",
    "\n",
    "\n",
    "    print(gru_node)\n",
    "    for i in range(0,hidden_layer):\n",
    "        model.add(GRU(gru_node,return_sequences=True, recurrent_dropout=0.2))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(GRU(gru_node, recurrent_dropout=0.2))\n",
    "    # model.add(Dense(, activation='relu'))\n",
    "    model.add(Dense(nclasses, activation='softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>twitter</th>\n",
       "      <th>party</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3105</th>\n",
       "      <td>Richard J. Durbin</td>\n",
       "      <td>SenatorDurbin</td>\n",
       "      <td>1</td>\n",
       "      <td>Facebook must work to stop the spread of misin...</td>\n",
       "      <td>2020-04-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21599</th>\n",
       "      <td>Gary C. Peters</td>\n",
       "      <td>SenGaryPeters</td>\n",
       "      <td>1</td>\n",
       "      <td>It’s alarming that HHS workers reportedly rece...</td>\n",
       "      <td>2020-02-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4411</th>\n",
       "      <td>Jeanne Shaheen</td>\n",
       "      <td>SenatorShaheen</td>\n",
       "      <td>1</td>\n",
       "      <td>I’m glad the President heeded our concerns in ...</td>\n",
       "      <td>2020-05-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4803</th>\n",
       "      <td>Tom Udall</td>\n",
       "      <td>SenatorTomUdall</td>\n",
       "      <td>1</td>\n",
       "      <td>We are calling for a smart economic stimulus p...</td>\n",
       "      <td>2020-03-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58125</th>\n",
       "      <td>Andy Kim</td>\n",
       "      <td>RepAndyKimNJ</td>\n",
       "      <td>1</td>\n",
       "      <td>The time to fully activate the Defense Product...</td>\n",
       "      <td>2020-03-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name          twitter party  \\\n",
       "3105   Richard J. Durbin    SenatorDurbin     1   \n",
       "21599     Gary C. Peters    SenGaryPeters     1   \n",
       "4411      Jeanne Shaheen   SenatorShaheen     1   \n",
       "4803           Tom Udall  SenatorTomUdall     1   \n",
       "58125           Andy Kim     RepAndyKimNJ     1   \n",
       "\n",
       "                                                   tweet tweet_published  \n",
       "3105   Facebook must work to stop the spread of misin...      2020-04-07  \n",
       "21599  It’s alarming that HHS workers reportedly rece...      2020-02-28  \n",
       "4411   I’m glad the President heeded our concerns in ...      2020-05-28  \n",
       "4803   We are calling for a smart economic stimulus p...      2020-03-11  \n",
       "58125  The time to fully activate the Defense Product...      2020-03-19  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nn = df.copy()\n",
    "for _,row in df_nn.iterrows():\n",
    "    row['party'] = 1 if row['party'] == 'Democrat' else 0;\n",
    "df_nn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26631    0\n",
      "12330    0\n",
      "34033    0\n",
      "16432    1\n",
      "50996    1\n",
      "        ..\n",
      "30166    1\n",
      "5756     0\n",
      "39772    0\n",
      "2453     0\n",
      "15992    1\n",
      "Name: party, Length: 31983, dtype: object\n",
      "53550    1\n",
      "26899    0\n",
      "34476    1\n",
      "2715     0\n",
      "38997    0\n",
      "        ..\n",
      "27851    1\n",
      "40277    0\n",
      "61093    1\n",
      "44170    0\n",
      "48201    1\n",
      "Name: party, Length: 10661, dtype: object\n",
      "10661\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "vect = CountVectorizer(max_features=10000)\n",
    "vect.fit(X_train.values.astype('U'))\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "Y_train_glove = vect.transform(Y_train)\n",
    "Y_test_glove  = vect.transform(Y_test)\n",
    "print(Y_train_glove)\n",
    "'''\n",
    "\n",
    "X_train_nn, X_test_nn, Y_train_nn, Y_test_nn = train_test_split(df_nn['tweet'], df_nn['party'], test_size = 0.25, random_state = 42, stratify=df['party'])\n",
    "Y_train_glove = Y_train_nn.copy()\n",
    "print(Y_train_glove)\n",
    "# Y_train_glove = np.where(Y_train_glove == 'Democrat', 1, -1)\n",
    "# print(Y_train_glove)\n",
    "\n",
    "Y_test_glove = Y_test_nn.copy()\n",
    "print(Y_test_glove)\n",
    "print(len(Y_test_glove))\n",
    "# Y_test_glove = np.where(Y_test_glove == 'Democrat', 1, -1)\n",
    "# print(Y_test_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42697 unique tokens.\n",
      "(42644, 50)\n",
      "Total 400000 word vectors.\n",
      "256\n",
      "WARNING:tensorflow:Layer gru_8 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_10 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_11 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 50)            2134900   \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (None, 50, 256)           236544    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, 50, 256)           394752    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "gru_10 (GRU)                 (None, 50, 256)           394752    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (None, 256)               394752    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10634)             2732938   \n",
      "=================================================================\n",
      "Total params: 6,288,638\n",
      "Trainable params: 6,288,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train_nn,X_test_nn)\n",
    "\n",
    "\n",
    "model_RNN = Build_Model_RNN_Text(word_index,embeddings_index, len(np.unique(X_test_nn)))\n",
    "\n",
    "model_RNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if not tf.test.gpu_device_name(): \n",
    "    warnings.warn('No GPU found')\n",
    "else: \n",
    "    print('Default GPU device: {}' .format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_Glove = np.asarray(X_train_Glove).astype('float32')\n",
    "X_test_Glove = np.asarray(X_test_Glove).astype('float32')\n",
    "Y_train_glove = np.asarray(Y_train_glove).astype('float32')\n",
    "Y_test_glove = np.asarray(Y_test_glove).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1000/1000 [==============================] - 1579s 2s/step - loss: 0.7271 - accuracy: 0.5006 - val_loss: 0.6939 - val_accuracy: 0.4979\n",
      "Epoch 2/15\n",
      "1000/1000 [==============================] - 705s 705ms/step - loss: 0.7007 - accuracy: 0.4996 - val_loss: 0.6971 - val_accuracy: 0.4979\n",
      "Epoch 3/15\n",
      "1000/1000 [==============================] - 714s 714ms/step - loss: 0.7017 - accuracy: 0.5040 - val_loss: 0.6986 - val_accuracy: 0.4979\n",
      "Epoch 4/15\n",
      " 156/1000 [===>..........................] - ETA: 9:37 - loss: 0.7025 - accuracy: 0.4984"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-ace1024ff18e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                               \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                               shuffle=True)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tim\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tim\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tim\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tim\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tim\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tim\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tim\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tim\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tim\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_RNN.fit(X_train_Glove, Y_train_glove,\n",
    "                              validation_data=(X_test_Glove, Y_test_glove),\n",
    "                              epochs=15,\n",
    "                              batch_size=32,\n",
    "                              verbose=1,\n",
    "                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10661"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_Glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of unknown and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-8446aeb6557a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test_nn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tim\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tim\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   1927\u001b[0m     \"\"\"\n\u001b[0;32m   1928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1929\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1931\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tim\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 91\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of unknown and binary targets"
     ]
    }
   ],
   "source": [
    "predicted = model_RNN.predict_classes(X_test_Glove)\n",
    "\n",
    "\n",
    "print(metrics.classification_report(Y_test_nn, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where((Y_test_nn != 1) & (Y_test_nn != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
