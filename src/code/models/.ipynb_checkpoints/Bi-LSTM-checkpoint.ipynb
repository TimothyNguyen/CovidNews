{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in c:\\users\\quynh\\anaconda3\\lib\\site-packages (0.5.10)\n",
      "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from num2words) (0.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install num2words\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('covid_tweets.csv')\n",
    "democrat_tweets_df = df[df.party == 'Democrat']\n",
    "republican_tweets_df = df[df.party == 'Republican']\n",
    "democrat_tweets_df = democrat_tweets_df.sample(frac = 0.5)\n",
    "frames = [democrat_tweets_df, republican_tweets_df]\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>twitter</th>\n",
       "      <th>party</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27716</th>\n",
       "      <td>Kamala D. Harris</td>\n",
       "      <td>SenKamalaHarris</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Farm workers have always been essential, and i...</td>\n",
       "      <td>2020-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36413</th>\n",
       "      <td>Debbie Mucarsel-Powell</td>\n",
       "      <td>RepDMP</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>COVID-19 is surging. Floridians are frustrated...</td>\n",
       "      <td>2020-06-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33773</th>\n",
       "      <td>Joseph D. Morelle</td>\n",
       "      <td>RepJoeMorelle</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>I’m live on @warm1013 with my friend @TonyWarm...</td>\n",
       "      <td>2020-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18003</th>\n",
       "      <td>Nydia M. Velázquez</td>\n",
       "      <td>NydiaVelazquez</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>This #OlderAmericansMonth we celebrate the con...</td>\n",
       "      <td>2020-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42265</th>\n",
       "      <td>Elaine G. Luria</td>\n",
       "      <td>RepElaineLuria</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Recently, @Apple collaborated with @CDCgov to ...</td>\n",
       "      <td>2020-04-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name          twitter     party  \\\n",
       "27716        Kamala D. Harris  SenKamalaHarris  Democrat   \n",
       "36413  Debbie Mucarsel-Powell           RepDMP  Democrat   \n",
       "33773       Joseph D. Morelle    RepJoeMorelle  Democrat   \n",
       "18003      Nydia M. Velázquez   NydiaVelazquez  Democrat   \n",
       "42265         Elaine G. Luria   RepElaineLuria  Democrat   \n",
       "\n",
       "                                                   tweet tweet_published  \n",
       "27716  Farm workers have always been essential, and i...      2020-04-08  \n",
       "36413  COVID-19 is surging. Floridians are frustrated...      2020-06-29  \n",
       "33773  I’m live on @warm1013 with my friend @TonyWarm...      2020-04-10  \n",
       "18003  This #OlderAmericansMonth we celebrate the con...      2020-05-26  \n",
       "42265  Recently, @Apple collaborated with @CDCgov to ...      2020-04-03  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None )\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "covid_stop_words = {\"covid\", \"covid19\", \"covid 19\", \"corona\", \"coronavirus\"}\n",
    "porter = PorterStemmer()\n",
    "def preprocess_and_tokenize(tweet):\n",
    "    # Remove non-letters, lowercase everything, remove stop words, and stem\n",
    "    lower_letters = re.sub(r'[^A-Za-z0-9 ]+', \" \", tweet).lower().split()\n",
    "    important_words = []\n",
    "    for w in lower_letters:\n",
    "        if w not in stop_words and w not in covid_stop_words:\n",
    "            important_words.append(w)\n",
    "    return important_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenize_tweet'] = df['tweet'].apply(lambda x : preprocess_and_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dems, reps = list(), list()\n",
    "for l in df.party:\n",
    "    if l == 'Democrat':\n",
    "        dems.append(1)\n",
    "        reps.append(0)\n",
    "    elif l == 'Republican':\n",
    "        reps.append(1)\n",
    "        dems.append(0)\n",
    "df['dems']= dems\n",
    "df['reps']= reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>twitter</th>\n",
       "      <th>party</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_published</th>\n",
       "      <th>tokenize_tweet</th>\n",
       "      <th>dems</th>\n",
       "      <th>reps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27716</th>\n",
       "      <td>Kamala D. Harris</td>\n",
       "      <td>SenKamalaHarris</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Farm workers have always been essential, and i...</td>\n",
       "      <td>2020-04-08</td>\n",
       "      <td>[farm, workers, always, essential, long, past,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36413</th>\n",
       "      <td>Debbie Mucarsel-Powell</td>\n",
       "      <td>RepDMP</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>COVID-19 is surging. Floridians are frustrated...</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>[covid, 19, surging, floridians, frustrated, w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33773</th>\n",
       "      <td>Joseph D. Morelle</td>\n",
       "      <td>RepJoeMorelle</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>I’m live on @warm1013 with my friend @TonyWarm...</td>\n",
       "      <td>2020-04-10</td>\n",
       "      <td>[live, warm1013, friend, tonywarm1013, talking...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18003</th>\n",
       "      <td>Nydia M. Velázquez</td>\n",
       "      <td>NydiaVelazquez</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>This #OlderAmericansMonth we celebrate the con...</td>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>[olderamericansmonth, celebrate, contributions...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42265</th>\n",
       "      <td>Elaine G. Luria</td>\n",
       "      <td>RepElaineLuria</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Recently, @Apple collaborated with @CDCgov to ...</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>[recently, apple, collaborated, cdcgov, develo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name          twitter     party  \\\n",
       "27716        Kamala D. Harris  SenKamalaHarris  Democrat   \n",
       "36413  Debbie Mucarsel-Powell           RepDMP  Democrat   \n",
       "33773       Joseph D. Morelle    RepJoeMorelle  Democrat   \n",
       "18003      Nydia M. Velázquez   NydiaVelazquez  Democrat   \n",
       "42265         Elaine G. Luria   RepElaineLuria  Democrat   \n",
       "\n",
       "                                                   tweet tweet_published  \\\n",
       "27716  Farm workers have always been essential, and i...      2020-04-08   \n",
       "36413  COVID-19 is surging. Floridians are frustrated...      2020-06-29   \n",
       "33773  I’m live on @warm1013 with my friend @TonyWarm...      2020-04-10   \n",
       "18003  This #OlderAmericansMonth we celebrate the con...      2020-05-26   \n",
       "42265  Recently, @Apple collaborated with @CDCgov to ...      2020-04-03   \n",
       "\n",
       "                                          tokenize_tweet  dems  reps  \n",
       "27716  [farm, workers, always, essential, long, past,...     1     0  \n",
       "36413  [covid, 19, surging, floridians, frustrated, w...     1     0  \n",
       "33773  [live, warm1013, friend, tonywarm1013, talking...     1     0  \n",
       "18003  [olderamericansmonth, celebrate, contributions...     1     0  \n",
       "42265  [recently, apple, collaborated, cdcgov, develo...     1     0  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test = train_test_split(df, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We build training vocab/max trailing sentence length/total number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727693 words total, with a vocabulary size of 32772\n",
      "Max sentence length is 92\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in data_train[\"tokenize_tweet\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokenize_tweet\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build testing vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241968 words total, with a vocabulary size of 19018\n",
      "Max sentence length is 77\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in data_test[\"tokenize_tweet\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokenize_tweet\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quynh\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "### Load up word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "word2vec.wv.save_word2vec_format('googlenews.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize & Pad the Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36654 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"tweet\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"tweet\"].tolist())\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "train_rnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will get embeddings from Google News Word2Vec model and save them corresponding to the sequence number we assigned to each word. If we could not get embeddings we save a random vector for that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36655, 300)\n"
     ]
    }
   ],
   "source": [
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"tweet\"].tolist())\n",
    "test_rnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['dems', 'reps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train[label_names].values\n",
    "x_train = train_rnn_data\n",
    "y_tr = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[]\n",
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.config.list_physical_devices('GPU')) \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurrent_nn(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    lstm = LSTM(256)(embedded_sequences)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(lstm)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None )\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 50, 300)           10996500  \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 256)               570368    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 11,600,022\n",
      "Trainable params: 603,522\n",
      "Non-trainable params: 10,996,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model = recurrent_nn(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM,  len(list(label_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "847/847 [==============================] - 317s 375ms/step - loss: 0.6280 - acc: 0.6403 - val_loss: 0.5849 - val_acc: 0.6715\n",
      "Epoch 2/5\n",
      "847/847 [==============================] - 356s 421ms/step - loss: 0.5553 - acc: 0.7142 - val_loss: 0.5254 - val_acc: 0.7352\n",
      "Epoch 3/5\n",
      "847/847 [==============================] - 363s 429ms/step - loss: 0.5133 - acc: 0.7421 - val_loss: 0.5126 - val_acc: 0.7362\n",
      "Epoch 4/5\n",
      "847/847 [==============================] - 366s 433ms/step - loss: 0.4692 - acc: 0.7728 - val_loss: 0.4962 - val_acc: 0.7555\n",
      "Epoch 5/5\n",
      "847/847 [==============================] - 305s 361ms/step - loss: 0.4174 - acc: 0.8038 - val_loss: 0.4882 - val_acc: 0.7702\n"
     ]
    }
   ],
   "source": [
    "hist = rnn_model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.10, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 14s 689ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = rnn_model.predict(test_rnn_data, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7487102523215459"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [1, 0]\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "sum(data_test['dems'] == prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a CNN\n",
    "# 1. Embeddings matrix passed to embedding layer\n",
    "# 2. Five different filter sizese are applied to each tweet\n",
    "# 3. GlobalMaxPooling1D layers are applied to each layer\n",
    "# 4. All outputs concatenated\n",
    "# 5. Dropout -> Dense -> Dropout -> Dense applied\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(embeddings, max_seq_length, num_words, \n",
    "        embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                               embedding_dim,\n",
    "                               weights=[embeddings],\n",
    "                               input_length=max_seq_length,\n",
    "                               trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "    \n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, \n",
    "                        kernel_size=filter_size, \n",
    "                        activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "        \n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "    \n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 50, 300)      10996500    input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 49, 200)      120200      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 48, 200)      180200      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 47, 200)      240200      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 46, 200)      300200      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 45, 200)      360200      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 200)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 200)          0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 200)          0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 200)          0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 200)          0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1000)         0           global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1000)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 128)          128128      dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128)          0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 2)            258         dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 12,325,886\n",
      "Trainable params: 1,329,386\n",
      "Non-trainable params: 10,996,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300\n",
    "cnn_model = cnn(train_embedding_weights, \n",
    "                MAX_SEQUENCE_LENGTH, \n",
    "                len(train_word_index)+1, \n",
    "                EMBEDDING_DIM, \n",
    "                len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 673/1440 [=============>................] - ETA: 22s - loss: 0.1803 - acc: 0.9298"
     ]
    }
   ],
   "source": [
    "hist = cnn_model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.10, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 4s 325ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = cnn_model.predict(test_rnn_data, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7736610074101866"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [1, 0]\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "sum(data_test['dems'] == prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
